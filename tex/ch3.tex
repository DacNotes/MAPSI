\documentclass{article}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath}

\title{Chapitre 3 \\ MLE \& MAP}
\author{VieVie31}

\begin{document}

\newtheorem{theo}{Théorème}
\newtheorem{coro}{Corollaire}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\likelihood}{{\cal L}}

\maketitle


\section{Vraisemblance d'un échantillon (loi discrète connue)}
$\likelihood(X) = $ vraisemblance de l'échantillon \textit{sachant la loi $P$}.

\begin{itemize}
    \item échantillon $\textbf{x} = (x_1, ..., x_n)$ de taille $n$
    \item échantillon : les $x_i = $ réalisation de variables aléatoires $X_i$
    \item les $X_i$ sont mutuellement indépendants (hypothèse essentielle)
\end{itemize}

\begin{equation}
    \likelihood(X) = P(x_1, ..., x_n) = \prod\limits_{i=1}^n P(X_i = x_i)
\end{equation} 


\section{Estimation par Maximum de Vraisemblance}

\begin{itemize}
    \item paramètre à estimer : $\Theta$
\end{itemize}

Calcul de vraisemblance de l'échantillon :
\begin{equation}
    \likelihood(x, \theta) = P(x_1, ..., x_n | \Theta=\theta) = \prod\limits_{i=1}^n P(x | \Theta = \theta)
\end{equation}

On cherche à maximiser $\theta$:
\begin{equation}
    \argmax\limits_{\theta \in \Theta} \likelihood(x, \Theta)
    \iff \argmax\limits_{\theta \in \Theta} \ln \likelihood(x, \Theta) 
    \iff \argmax\limits_{\theta \in \Theta} \sum\limits_{i=1}^n\ln P(x_i | \theta)
\end{equation}

Qui est obtenu losrque (sous conditaion de \textbf{concavité} et de dérivabilité):
\begin{equation}
    \frac{\partial \likelihood(x, \theta)}{\partial \theta} = 0
    \iff \frac{\partial \ln\likelihood(x, \theta)}{\partial \theta} = 0
    \iff \sum\limits_{i=1}^n \frac{\partial \ln \likelihood(x, \theta)}{\partial \theta} = 0
\end{equation}


\subsection{Calcul de l'Estimation de Vraisemblance}
Données : 
\begin{itemize}
    \item loi de probabilité $P(X = n) = f(x)$
    \item échantillon $(x_1, ..., x_n)$
\end{itemize}
Poser :
\begin{equation}
    \prod\limits^n P(X = n) = \prod\limits^n f(x)
\end{equation}
Puis calculer son logarithme :
\begin{equation}
    \sum\limits^n \ln P(X = n) = \sum\limits^n \ln f(x)
\end{equation}
Puis calculer sa dérivée partielle en fonction du paramètre $\theta$ et trouver où elle s'annule:
\begin{equation}
    \frac{\partial f(x)}{\partial \theta} = 0
\end{equation}



\section{Estimateurs de Vraisemblance de Lois connues}

\subsection{Bernoulli}

\begin{gather*}
    \likelihood(x, \theta) = \prod\limits^n \theta^{x_i}(1 - \theta)^{1 - x_i} \\ \\
    \ln \likelihood(x, \theta) = \sum\limits^n \ln(\theta^{x_i}(1 - \theta)^{1 - x_i}) \\
    = \sum\limits^n x_i (\ln\theta) + \sum\limits^n ((1-x_i) \ln(1-\theta)) \\ \\
    \frac{\partial \ln \likelihood(x, \theta)}{\partial \theta} = 0 \\
    \iff \frac{\sum\limits^n x_i}{\theta} - \frac{\sum\limits^n (1-x_i)}{1 - \theta} = 0 \\
    \iff \frac{(1 - \theta) \sum\limits^n x_i - \theta \sum\limits^n (1 - x_i)}{\theta (1 - \theta)} = 0 \\
    \iff \sum\limits^n x_i - \sum\limits^n x_i \theta - (n\theta - \sum\limits^n x_i\theta) = 0 \\
    \iff \sum\limits^n x_i - \sum\limits^n x_i \theta -  n\theta + \sum\limits^n x_i\theta  = 0 \\
    \iff \sum\limits^n x_i - n\theta = 0 \\
    \iff \frac{\sum\limits^n x_i}{n} = \theta
\end{gather*}

\subsection{Normale}

\begin{gather*}
        f(x) = (2 \pi \sigma^2)^{-1/2} \exp\Big(\frac{-1}{2} \frac{(x-\mu)}{\sigma^2}\Big)^2 \\ \\
        \likelihood(x, \theta) = \prod\limits^n f(x) \\ \\
        \ln\likelihood(x, \theta) = \sum\limits^n \ln f(x) \\
        = \sum\limits^n \ln(2 \pi \sigma^2)^{-1/2} + \sum\limits^n \Big(\frac{-1}{2} \frac{(x_i-\mu)}{\sigma^2})\Big)^2 \\
        = \sum\limits^n \frac{- 1}{2} \ln(2 \pi \sigma^2) - \frac{1}{2} \sum\limits^n \frac{(x_i -\mu)^2}{\sigma^2} \\
        = \frac{-n \ln(2 \pi \sigma^2)}{2} - \frac{1}{2 \sigma^2} \sum\limits^n (x_i - \mu)^2 \\
        = \frac{-n \ln(2 \pi)}{2} - {n \ln(\sigma)} - \frac{1}{2 \sigma^2} \sum\limits^n (x_i - \mu)^2
\end{gather*}

La loi normale $\mathcal{N}(\mu, \sigma^2)$ a deux paramètres $\mu$ et $\sigma^2$.
On trouve d'abord le premier paramètre $\mu$.

\begin{gather*}
    \frac{\partial \likelihood(x, \theta)}{\partial \mu} = \frac{\partial \frac{-1}{2 \sigma^2} \sum\limits^n (x_i - \mu)^2}{\partial \mu} = 0 \\
    \iff \frac{\partial \frac{1}{2}\sum\limits^n (x_i-\mu)^2}{\partial \mu} = \sum\limits^n (x_i - \mu) = 0 \\
    \iff \sum\limits^n x_i - n\mu = 0 \\
    \iff \mu = \frac{1}{n} \sum\limits^n x_i
\end{gather*}

Puis le paramètre $\sigma$.

\begin{gather*}
    \frac{\partial \likelihood(x, \theta)}{\partial \sigma} = 0 \\
    \iff \frac{-n}{2} \frac{2}{\sigma} - \frac{\sum\limits^n (x_i i \mu)^2}{2} \frac{-2}{\sigma^3}  = 0\\
    \iff -n + \frac{\sum\limits^n (x_i - \mu)^2}{\sigma^2} = 0
    \iff \frac{1}{n} \sum\limits^n (x_i - \mu)^2 = \sigma^2
\end{gather*}

\subsection{Distribution Exponentielle}

\begin{gather*}
    f(x) = \begin{cases}
        \lambda \exp(-\lambda x), & \text{si $x \geq 0$}.\\
        0, & \text{sinon}.
      \end{cases}
    \\ \\ 
    \likelihood(x, \theta) = \prod\limits^n \theta \exp(-\theta x_i) \\ \\
    \ln \likelihood(x, \theta) = \sum\limits^n \ln(\theta \exp(-\theta x_i)) = n\ln\theta - \theta \sum\limits^n x_i \\ \\
    \frac{\partial \ln \likelihood(x, \theta)}{\theta} = \frac{n}{\theta} -\sum\limits^n x_i = 0 \\
    \iff -\sum\limits^n x_i = \frac{-n}{\theta} \\
    \iff \theta = \frac{n}{\sum\limits^n x_i}
\end{gather*}

\subsection{Loi Géométrique}

TODO



\section{Maximum a Posteriori}

\subsection{Théorème de Bayes}

\begin{equation}
    P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}

Avec : 
\begin{itemize}
    \item $P(A|B)$ : posterior
    \item $P(B|A)$ : likelihood
    \item $P(A)$ : prior
    \item $P(B)$ : evidence
\end{itemize}

Dans notre cas :
\begin{itemize}
    \item $A = \theta$ (le modèle)
    \item $B = X$ (observations)
\end{itemize}


\subsection{Maximum a Posteriori}

\begin{align*}
    \hat\theta_{MAP} & = \argmax\limits_{\theta} P(\theta | X) \\
    & = \argmax\limits_{\theta} \frac{P(X | \theta)P(\theta)}{P(X)} \\
    & \propto \argmax\limits_{\theta} P(X | \theta)P(\theta) \\
    & = \argmax\limits_{\theta} \prod\limits^n P(x_i | \theta)P(\theta) \\
    & = \argmax\limits_{\theta} \ln P(\theta | X) \\
    & = \argmax\limits_{\theta} \ln \prod\limits^n P(x_i | \theta)P(\theta) \\
    & = \argmax\limits_{\theta} \sum\limits^n (\ln(P(x_i | \theta)) + \ln(P(\theta)))
\end{align*}

Il suffit donc de calculer : 
\begin{equation*}
    \frac{
        \partial \ln \prod\limits^n P(x_i | \theta)P(\theta)
    }{
        \partial \theta
    } = 0
\end{equation*}


\end{document}

